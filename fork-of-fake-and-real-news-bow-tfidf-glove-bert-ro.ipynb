{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Read True and Fake CSV","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:17:34.609981Z","iopub.execute_input":"2022-01-27T10:17:34.610587Z","iopub.status.idle":"2022-01-27T10:17:34.638695Z","shell.execute_reply.started":"2022-01-27T10:17:34.610488Z","shell.execute_reply":"2022-01-27T10:17:34.637945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_df = pd.read_csv('../input/fake-and-real-news-dataset/True.csv')\ntrue_df.head()  # 21417 rows × 4 columns","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:17:34.642244Z","iopub.execute_input":"2022-01-27T10:17:34.642703Z","iopub.status.idle":"2022-01-27T10:17:36.454739Z","shell.execute_reply.started":"2022-01-27T10:17:34.64266Z","shell.execute_reply":"2022-01-27T10:17:36.454021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_df = pd.read_csv('../input/fake-and-real-news-dataset/Fake.csv')\nfake_df.head()  # 23481 rows × 4 columns","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:17:36.456114Z","iopub.execute_input":"2022-01-27T10:17:36.456369Z","iopub.status.idle":"2022-01-27T10:17:37.875691Z","shell.execute_reply.started":"2022-01-27T10:17:36.456336Z","shell.execute_reply":"2022-01-27T10:17:37.875024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Add Label Column and create concat dataframe","metadata":{}},{"cell_type":"code","source":"## Assign labels for the news type\ntrue_df['news_type'] = 1 \nfake_df['news_type'] = 0","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:17:37.877795Z","iopub.execute_input":"2022-01-27T10:17:37.87805Z","iopub.status.idle":"2022-01-27T10:17:37.88562Z","shell.execute_reply.started":"2022-01-27T10:17:37.878015Z","shell.execute_reply":"2022-01-27T10:17:37.884685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## concate both dataframes\nnews_df = pd.concat([true_df,fake_df],ignore_index=True)\nnews_df = news_df.sample(frac=1).reset_index(drop=True)\ntrain_df = news_df # for future use\ntrain_df.head()  # 44898 rows × 5 columns","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:17:37.887092Z","iopub.execute_input":"2022-01-27T10:17:37.887631Z","iopub.status.idle":"2022-01-27T10:17:37.923232Z","shell.execute_reply.started":"2022-01-27T10:17:37.887592Z","shell.execute_reply":"2022-01-27T10:17:37.922521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfinal_labels = np.array(news_df['news_type'])","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:17:37.924414Z","iopub.execute_input":"2022-01-27T10:17:37.925314Z","iopub.status.idle":"2022-01-27T10:17:38.933733Z","shell.execute_reply.started":"2022-01-27T10:17:37.925263Z","shell.execute_reply":"2022-01-27T10:17:38.933026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess Text","metadata":{}},{"cell_type":"code","source":"import string\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('stopwords')\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import *","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:17:38.935805Z","iopub.execute_input":"2022-01-27T10:17:38.9366Z","iopub.status.idle":"2022-01-27T10:17:40.579498Z","shell.execute_reply.started":"2022-01-27T10:17:38.936559Z","shell.execute_reply":"2022-01-27T10:17:40.57872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_df.iloc[0]['text']","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:17:40.580773Z","iopub.execute_input":"2022-01-27T10:17:40.581242Z","iopub.status.idle":"2022-01-27T10:17:40.589411Z","shell.execute_reply.started":"2022-01-27T10:17:40.581203Z","shell.execute_reply":"2022-01-27T10:17:40.588051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Preprocess the Text\n\nwordnet_lemma = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    \n    # Tokenization\n    tokens = word_tokenize(text)\n    \n    # lower\n    tokens = [w.lower() for w in tokens]\n    \n    # stemming\n    stem_ls = [PorterStemmer().stem(w) for w in tokens]\n    \n    # lemmatization\n    lemma_ls = [wordnet_lemma.lemmatize(w) for w in stem_ls]\n    \n    # remove punctuation\n    stripped_ls = [w for w in lemma_ls if not w in string.punctuation]\n\n    # remove tokens that are not alphabetic or numeric\n    words = [word for word in stripped_ls if word.isalpha() or word.isnumeric()]\n    \n    # removing stopwords\n    words = [w for w in words if not w in stop_words]\n    \n    return ' '.join(words)\n\npreprocess_text(news_df.iloc[0]['text'])","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:17:40.590927Z","iopub.execute_input":"2022-01-27T10:17:40.591206Z","iopub.status.idle":"2022-01-27T10:17:42.61767Z","shell.execute_reply.started":"2022-01-27T10:17:40.59117Z","shell.execute_reply":"2022-01-27T10:17:42.617013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nnews_df['text'] = news_df['text'].apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:17:42.62078Z","iopub.execute_input":"2022-01-27T10:17:42.621023Z","iopub.status.idle":"2022-01-27T10:31:45.967052Z","shell.execute_reply.started":"2022-01-27T10:17:42.620993Z","shell.execute_reply":"2022-01-27T10:31:45.965486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:31:45.968348Z","iopub.execute_input":"2022-01-27T10:31:45.968588Z","iopub.status.idle":"2022-01-27T10:31:45.978595Z","shell.execute_reply.started":"2022-01-27T10:31:45.968555Z","shell.execute_reply":"2022-01-27T10:31:45.97792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## WORDCLOUD for True News","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud,STOPWORDS\nimport matplotlib.pyplot as plt \n\nplt.figure(figsize=(15,15))\n\nwc_true = WordCloud(max_words = 2000, width=1000, height=500, stopwords= STOPWORDS).generate(' '.join(news_df[news_df['news_type']==1].text))\nplt.imshow(wc_true, interpolation = 'bilinear')","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:31:45.97991Z","iopub.execute_input":"2022-01-27T10:31:45.980345Z","iopub.status.idle":"2022-01-27T10:32:21.389998Z","shell.execute_reply.started":"2022-01-27T10:31:45.980309Z","shell.execute_reply":"2022-01-27T10:32:21.389028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## WORDCLOUD for Fake News","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\n\nwc_fake = WordCloud(max_words = 2000, width=1000, height=500, stopwords= STOPWORDS).generate(' '.join(news_df[news_df['news_type']==0].text))\nplt.imshow(wc_fake, interpolation = 'bilinear')","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:32:21.391421Z","iopub.execute_input":"2022-01-27T10:32:21.392113Z","iopub.status.idle":"2022-01-27T10:32:59.540413Z","shell.execute_reply.started":"2022-01-27T10:32:21.392075Z","shell.execute_reply":"2022-01-27T10:32:59.53976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Case 1 : Encoding words to numbers using Bag of Words with sklearn:SVM and sklearn:RandomForest Classifier","metadata":{}},{"cell_type":"markdown","source":"### Bag Of Words","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorize = CountVectorizer(max_features=200,ngram_range=(1,3))\ndata_1 = vectorize.fit_transform(news_df['text']).toarray()\n\nprint(\"Data Case 1 : \\n\",data_1)\nprint(\"Data Case 1 Shape : \\n\",data_1.shape)\n\nprint(\"Label : \\n\",final_labels)\nprint(\"Label Shape : \\n\",final_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:32:59.5414Z","iopub.execute_input":"2022-01-27T10:32:59.541732Z","iopub.status.idle":"2022-01-27T10:34:32.643478Z","shell.execute_reply.started":"2022-01-27T10:32:59.541702Z","shell.execute_reply":"2022-01-27T10:34:32.64276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(data_1,final_labels,test_size=0.2,random_state=777)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:34:32.644866Z","iopub.execute_input":"2022-01-27T10:34:32.645111Z","iopub.status.idle":"2022-01-27T10:34:32.682574Z","shell.execute_reply.started":"2022-01-27T10:34:32.645077Z","shell.execute_reply":"2022-01-27T10:34:32.681775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### sklearn : SVM Classifier","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.svm import LinearSVC\n\nprint(\"\\nSVM Classifier : \\n\")\nmodel_svc = LinearSVC().fit(x_train, y_train)\ny_pred = model_svc.predict(x_test)\n\nprint(\"Confusion Matrix : \\n\",confusion_matrix(y_test,y_pred))\nprint(\"\\n\\nClassification Report : \\n\", classification_report(y_test,y_pred))\nprint(\"\\n\\nAccuracy : \",model_svc.score(x_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:34:32.684094Z","iopub.execute_input":"2022-01-27T10:34:32.684385Z","iopub.status.idle":"2022-01-27T10:34:35.869799Z","shell.execute_reply.started":"2022-01-27T10:34:32.684345Z","shell.execute_reply":"2022-01-27T10:34:35.866608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### sklearn : RandomForest Classifier","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_rfc = RandomForestClassifier(n_estimators=300,random_state=40).fit(x_train, y_train)\ny_pred = model_rfc.predict(x_test)\n\nprint(\"\\nRandomForestClassifier : \\n\")\nprint(\"Confusion Matrix : \\n\",confusion_matrix(y_test,y_pred))\nprint(\"\\nClassification Report : \\n\", classification_report(y_test,y_pred))\nprint(\"\\nAccuracy : \",model_rfc.score(x_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:34:35.870982Z","iopub.execute_input":"2022-01-27T10:34:35.871243Z","iopub.status.idle":"2022-01-27T10:34:54.758Z","shell.execute_reply.started":"2022-01-27T10:34:35.871209Z","shell.execute_reply":"2022-01-27T10:34:54.757191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Case 2 : TF-IDF with XGBoost and LightGBM Classifier","metadata":{}},{"cell_type":"markdown","source":"### TF-TDF ","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(lowercase=False, stop_words='english')\ndata_2 = tfidf.fit_transform(news_df['text'])\n\nprint(\"Data Case 2 : \\n\",data_2)\nprint(\"Data Case 2 Shape : \\n\",data_2.shape)\n\nprint(\"Label : \\n\",final_labels)\nprint(\"Label Shape : \\n\",final_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:34:54.759397Z","iopub.execute_input":"2022-01-27T10:34:54.76144Z","iopub.status.idle":"2022-01-27T10:35:05.775882Z","shell.execute_reply.started":"2022-01-27T10:34:54.761391Z","shell.execute_reply":"2022-01-27T10:35:05.775077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(data_1,final_labels,test_size=0.2,random_state=777)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:35:05.780218Z","iopub.execute_input":"2022-01-27T10:35:05.783057Z","iopub.status.idle":"2022-01-27T10:35:05.828867Z","shell.execute_reply.started":"2022-01-27T10:35:05.782992Z","shell.execute_reply":"2022-01-27T10:35:05.827948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost Classifier","metadata":{}},{"cell_type":"code","source":"%%time\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n\nmodel_xgb = XGBClassifier(eval_metric='rmse', use_label_encode=False).fit(x_train, y_train)\ny_pred = model_xgb.predict(x_test)\n\nprint(\"\\nXGBoost Classifier : \\n\")\nprint(\"Confusion Matrix : \\n\",confusion_matrix(y_test,y_pred))\nprint(\"\\nClassification Report : \\n\", classification_report(y_test,y_pred))\nprint(\"\\nAccuracy : \",accuracy_score(y_pred,y_test))","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:35:05.833302Z","iopub.execute_input":"2022-01-27T10:35:05.833785Z","iopub.status.idle":"2022-01-27T10:35:33.778547Z","shell.execute_reply.started":"2022-01-27T10:35:05.833747Z","shell.execute_reply":"2022-01-27T10:35:33.777817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LightGBM Classifier","metadata":{}},{"cell_type":"code","source":"%%time\nfrom lightgbm import LGBMClassifier\n\nx_train, x_val, y_train, y_val = train_test_split(x_train,y_train,test_size=0.2,random_state=777)\n\nprint(\"\\nLightGBM Classifier : \\n\")\nmodel_svc = LGBMClassifier(n_estimators = 300).fit(x_train, y_train, early_stopping_rounds=100, eval_metric='accuracy',eval_set=[(x_val,y_val)])\ny_pred = model_svc.predict(x_test)\n\nprint(\"Confusion Matrix : \\n\",confusion_matrix(y_test,y_pred))\nprint(\"\\n\\nClassification Report : \\n\", classification_report(y_test,y_pred))\nprint(\"\\n\\nAccuracy : \",accuracy_score(y_pred,y_test))","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:35:33.779954Z","iopub.execute_input":"2022-01-27T10:35:33.780339Z","iopub.status.idle":"2022-01-27T10:35:36.548759Z","shell.execute_reply.started":"2022-01-27T10:35:33.780303Z","shell.execute_reply":"2022-01-27T10:35:36.547838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Case 3 :  Pre-trained GloVe Embedding and Tensorflow LSTM","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nphysical_devices = tf.config.list_physical_devices('GPU')\ntf.config.experimental.set_memory_growth(physical_devices[0], enable=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:35:36.549899Z","iopub.execute_input":"2022-01-27T10:35:36.550137Z","iopub.status.idle":"2022-01-27T10:35:41.70647Z","shell.execute_reply.started":"2022-01-27T10:35:36.550101Z","shell.execute_reply":"2022-01-27T10:35:41.70455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenization, Padding","metadata":{}},{"cell_type":"code","source":"%%time\n# Tokenization : Representing each word by a vector\nfrom tensorflow.keras.preprocessing import text, sequence\n\nmax_features = 10000\nmaxlen = 300 # keep all text to 300, add padding for text len < 300 and truncating long ones\n\nX = news_df['text'].values\ntokenizer = text.Tokenizer(num_words = max_features)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\ndata_3 = sequence.pad_sequences(X,maxlen=maxlen)\n\nprint(\"Data Case 3 : \\n\",data_3)\nprint(\"Data Case 3 Shape : \\n\",data_3.shape)\n\nprint(\"Label : \\n\",final_labels)\nprint(\"Label Shape : \\n\",final_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:35:41.708143Z","iopub.execute_input":"2022-01-27T10:35:41.70842Z","iopub.status.idle":"2022-01-27T10:35:55.991629Z","shell.execute_reply.started":"2022-01-27T10:35:41.708384Z","shell.execute_reply":"2022-01-27T10:35:55.989826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Introducing GloVe Embedding and creating Embedding Matrix","metadata":{}},{"cell_type":"code","source":"%%time\n\nEmbedding_file ='../input/glove-twitter/glove.twitter.27B.100d.txt'\n\ndef get_coeffs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\nembed_id = dict(get_coeffs(*o.rstrip().rsplit(' ')) for o in open(Embedding_file, encoding=\"utf8\"))\n\nembeds = np.stack(embed_id.values())\nemb_mean, emb_std = embeds.mean(), embeds.std()\nembeds_len = embeds.shape[1]\n\nword_index = tokenizer.word_index # mapping of original word to number\nnb_words = min(max_features, len(word_index))\n\nembed_mat = np.random.normal(emb_mean, emb_std, (nb_words, embeds_len))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embed_vec = embed_id.get(word)\n    if embed_vec is not None: embed_mat[i] = embed_vec\n        \nprint(\"Embedding Matrix : \\n\",embed_mat)\nprint(\"Embedding Matrix size : \",embed_mat.shape)\n\n### Reference : https://www.kaggle.com/madz2000/nlp-using-glove-embeddings-99-87-accuracy --- to understand glove embedding","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:46:48.525658Z","iopub.execute_input":"2022-01-27T10:46:48.525947Z","iopub.status.idle":"2022-01-27T10:47:28.816727Z","shell.execute_reply.started":"2022-01-27T10:46:48.525916Z","shell.execute_reply":"2022-01-27T10:47:28.815833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build Model BiLSTM","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras import optimizers, Input, Model\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Masking, Bidirectional, Activation, BatchNormalization\nfrom tensorflow.keras.regularizers import l2, l1_l2\nfrom tensorflow.keras.callbacks import EarlyStopping\n\noptimizer=optimizers.Adam(clipnorm=0.25,lr=0.0005)\n\ndef BiLSTM_Model():\n    model = Sequential([\n    Embedding(max_features, output_dim = embeds_len, weights = [embed_mat], input_length=maxlen, trainable=False),\n    Bidirectional(LSTM(64,return_sequences=True,recurrent_regularizer=l2(2e-4))),\n    Dropout(0.1),\n    Bidirectional(LSTM(32,recurrent_regularizer=l2(2e-4),return_sequences=False)),\n    Dense(16,activation='relu'),\n    Dense(1,kernel_regularizer=l1_l2(1e-4, 2e-4),activation='sigmoid')\n    ])\n    model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n    return model\n\n\nmodel_bilstm = BiLSTM_Model()\nmodel_bilstm.summary()\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:47:37.538903Z","iopub.execute_input":"2022-01-27T10:47:37.539173Z","iopub.status.idle":"2022-01-27T10:47:42.234667Z","shell.execute_reply.started":"2022-01-27T10:47:37.539131Z","shell.execute_reply":"2022-01-27T10:47:42.233858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Model","metadata":{}},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(data_3,final_labels,test_size=0.2,random_state = 777)\nepochs = 10\nbatch_size = 128\n\nx_train,x_val,y_train,y_val = train_test_split(x_train,y_train,test_size=0.2,random_state = 777)\n\nhistory = model_bilstm.fit(x_train, y_train, batch_size = batch_size , validation_data = (x_val,y_val) , epochs = epochs , callbacks = [early_stopping])","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:47:58.377652Z","iopub.execute_input":"2022-01-27T10:47:58.378459Z","iopub.status.idle":"2022-01-27T10:51:26.143521Z","shell.execute_reply.started":"2022-01-27T10:47:58.378412Z","shell.execute_reply":"2022-01-27T10:51:26.142679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate Model","metadata":{}},{"cell_type":"code","source":"loss,acc = model_bilstm.evaluate(x_test,y_test)\n\nprint(\"\\n\\nTest Data Loss : \",loss*100)\nprint(\"\\nTest Data Accuracy : \",acc*100)\n\ny_pred = (model_bilstm.predict(x_test) > 0.5).astype(\"int32\")\n\nprint(\"\\nConfusion Matrix : \\n\",confusion_matrix(y_test,y_pred))\nprint(\"\\nClassification Report : \\n\", classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:51:37.005591Z","iopub.execute_input":"2022-01-27T10:51:37.006328Z","iopub.status.idle":"2022-01-27T10:51:46.520502Z","shell.execute_reply.started":"2022-01-27T10:51:37.00629Z","shell.execute_reply":"2022-01-27T10:51:46.51977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Case 4 : BERT and RoBERTa","metadata":{}},{"cell_type":"code","source":"import torch\n\nis_cuda = torch.cuda.is_available()\nif is_cuda:\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n    \nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:51:52.397843Z","iopub.execute_input":"2022-01-27T10:51:52.398108Z","iopub.status.idle":"2022-01-27T10:51:53.543114Z","shell.execute_reply.started":"2022-01-27T10:51:52.398078Z","shell.execute_reply":"2022-01-27T10:51:53.54235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initialize Pretrained Models of BERT and RoBERTa","metadata":{}},{"cell_type":"code","source":"%%time\n### Initialize Pretrained Models of BERT and RoBERTa\n\nsentences = train_df['text'].values\n\nfrom transformers import BertForSequenceClassification, BertTokenizer, RobertaForSequenceClassification, RobertaTokenizer, AdamW\n\nmodel_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2,output_attentions=False,output_hidden_states=False)\n# 'bert-base-uncased' : 12 layer BERT model with uncased vocab\n# num_labels : 2 labels for binary classification\ntokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel_bert.cuda()\n\nprint(\"\\n\\nBERT Model : \\n\\n\",model_bert)\n\nmodel_roberta = RobertaForSequenceClassification.from_pretrained('roberta-base',num_labels=2,output_attentions=False,output_hidden_states=False)\n# 'roberta-base' : 12 layer, 768 hidden, 12 heads, 125M params RoBERTa using BERT-base architecture\ntokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\nmodel_roberta.cuda()\n\nprint(\"\\n\\nRoBERTa Model : \\n\\n\",model_roberta)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:51:56.41971Z","iopub.execute_input":"2022-01-27T10:51:56.420304Z","iopub.status.idle":"2022-01-27T10:53:03.542529Z","shell.execute_reply.started":"2022-01-27T10:51:56.420265Z","shell.execute_reply":"2022-01-27T10:53:03.541725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenization : encode_plus method from tokenizer_bert and tokenizer_roberta ","metadata":{}},{"cell_type":"code","source":"%%time\n### Tokenization : encode_plus method from tokenizer_bert and tokenizer_roberta\n\n# encode plus : tokenize sentence, prepand [CLS] to start, append [SEP] to end, \n# map token to their ID, Pad or truncate the sentence to max_len, create attention masks for [PAD] tokens\n\ninputID_bert = []\nattentionMask_bert = []\n\ninputID_roberta = []\nattentionMask_roberta = []\n\nsentenceID = []\ncount = 0\n\nfor text in sentences:\n    \n    enc_dict_bert = tokenizer_bert.encode_plus(text,add_special_tokens=True,max_length=120,pad_to_max_length=True, return_attention_mask=True,return_tensors='pt')\n    enc_dict_roberta = tokenizer_roberta.encode_plus(text,add_special_tokens=True,max_length=120,pad_to_max_length=True, return_attention_mask=True,return_tensors='pt')\n    \n    # max_length : Pad and truncate all texts\n    # return_attention_mask : construct attention masks\n    # return_tensors : 'pt' : pytorch tensor\n    \n    inputID_bert.append(enc_dict_bert['input_ids'])\n    inputID_roberta.append(enc_dict_roberta['input_ids']) # added encoded text as ID to the list\n    \n    attentionMask_bert.append(enc_dict_bert['attention_mask']) # added attention mask to the list\n    attentionMask_roberta.append(enc_dict_roberta['attention_mask']) # that simply differs padding from non-padding\n\n    sentenceID.append(count)\n    count = count + 1\n    \n# convert lists to tensor\n\ninputID_bert = torch.cat(inputID_bert,dim=0)\ninputID_roberta = torch.cat(inputID_roberta,dim=0)\nattentionMask_bert = torch.cat(attentionMask_bert,dim=0)\nattentionMask_roberta = torch.cat(attentionMask_roberta,dim=0)\n\nlabels = torch.tensor(final_labels)\nsentenceID = torch.tensor(sentenceID)\n\nprint('\\nOriginal: \\n', sentences[0])\nprint('\\nToken IDs BERT: \\n', inputID_bert[0])\nprint('\\nToken IDs RoBERTa: \\n', inputID_roberta[0])\n\n### Reference : https://www.kaggle.com/jaskaransingh/fake-news-classification-bert-roberta --- for tokenization","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:53:22.418843Z","iopub.execute_input":"2022-01-27T10:53:22.419101Z","iopub.status.idle":"2022-01-27T11:01:56.075555Z","shell.execute_reply.started":"2022-01-27T10:53:22.419072Z","shell.execute_reply":"2022-01-27T11:01:56.074825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create DataSet and DataLoader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Remove sentence id from TensorDataset after train validation test split\ndef sid_remove_from_tensordataset(datatensor): \n    \n    inputID = []\n    attentionMask = []\n    label = []\n    \n    for sid,iid,amask,l in datatensor:\n        inputID.append(iid.tolist())\n        attentionMask.append(amask.tolist())\n        label.append(l.tolist())\n    \n    inputID = torch.tensor(inputID)\n    attentionMask = torch.tensor(attentionMask)\n    label = torch.tensor(label)\n    \n    return TensorDataset(inputID,attentionMask,label)\n    \n# Get DataSetLoaders\ndef get_loaders(dataset,batch_size,b):\n\n    \"\"\"\n    return the train, validation and test set loaders\n    \"\"\"  \n    #dataset = torch.utils.data.TensorDataset(data_tr, labels_tr)\n    train_size = int(0.8 * len(dataset))\n    val_size = int(0.1 * len(dataset))\n    test_size = len(dataset) - train_size - val_size\n    #print(\"\\nTrain DataSet Size :\",train_size)\n    #print(\"\\nValidation DataSet Size :\",val_size)\n    #print(\"\\nTest DataSet Size :\",test_size)\n    train_dataset, validation_dataset,test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n    \n    if(b==1): # sid remove only in BERT model : b=1\n        train_dataset = sid_remove_from_tensordataset(train_dataset)\n        validation_dataset = sid_remove_from_tensordataset(validation_dataset)\n        test_dataset = sid_remove_from_tensordataset(test_dataset)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size,sampler=RandomSampler(train_dataset))\n    valid_loader = DataLoader(validation_dataset, batch_size=batch_size,sampler=SequentialSampler(validation_dataset))    \n    test_loader = DataLoader(test_dataset, batch_size=batch_size,sampler=SequentialSampler(test_dataset))\n\n    return train_loader, valid_loader, test_loader\n","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:02:40.363905Z","iopub.execute_input":"2022-01-27T11:02:40.364179Z","iopub.status.idle":"2022-01-27T11:02:40.374294Z","shell.execute_reply.started":"2022-01-27T11:02:40.364128Z","shell.execute_reply":"2022-01-27T11:02:40.373545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train and Evaluate Model","metadata":{}},{"cell_type":"code","source":"def get_accuracy(y_pred, y_test):\n    y_pred_flat = np.argmax(y_pred, axis=1).flatten()\n    y_test_flat = y_test.flatten()\n    return np.sum(y_pred_flat == y_test_flat) / len(y_test_flat)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:02:44.040967Z","iopub.execute_input":"2022-01-27T11:02:44.041941Z","iopub.status.idle":"2022-01-27T11:02:44.047779Z","shell.execute_reply.started":"2022-01-27T11:02:44.041895Z","shell.execute_reply":"2022-01-27T11:02:44.047145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, optimizer, train_loader):\n  \n    model.train()\n\n    epoch_loss = 0\n    epoch_acc = 0\n\n    for iid, amask, labels in train_loader:\n        \n        iid, amask, labels = iid.to(device), amask.to(device), labels.to(device)\n        model.zero_grad()\n        loss,outputs = model(iid,token_type_ids=None, attention_mask=amask,labels=labels, return_dict=False)  \n        #torch.set_default_tensor_type(torch.FloatTensor)\n        \n        #x = torch.tensor(x, dtype=torch.float32)\n        #labels=torch.tensor(labels, dtype= torch.float32)\n        #loss = criterion(outputs, labels)\n        #loss = criterion(outputs.squeeze(), labels.float32)\n        epoch_loss += loss.item()\n        loss.backward()\n        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        \n        epoch_acc += get_accuracy(outputs.detach().cpu().numpy(),labels.to('cpu').numpy())\n\n    train_loss =  epoch_loss / len(train_loader)\n    train_acc = epoch_acc / len(train_loader)  \n    return train_loss, train_acc","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:02:46.272062Z","iopub.execute_input":"2022-01-27T11:02:46.272776Z","iopub.status.idle":"2022-01-27T11:02:46.279712Z","shell.execute_reply.started":"2022-01-27T11:02:46.272738Z","shell.execute_reply":"2022-01-27T11:02:46.278906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, loader):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    y_true=[]\n    y_pred=[]\n    \n    with torch.no_grad():\n    \n        for iid, amask, labels in loader:\n            iid, amask, labels = iid.to(device), amask.to(device), labels.to(device)\n\n            loss,outputs = model(iid,token_type_ids=None, attention_mask=amask, labels=labels, return_dict=False)\n            #torch.set_default_tensor_type(torch.FloatTensor)\n            labels=torch.tensor(labels, dtype= torch.float32)\n            \n            #e_loss = criterion(outputs.squeeze(), labels.float32)\n            #e_loss = criterion(outputs,labels)\n            epoch_loss += loss.item()\n            \n            epoch_acc += get_accuracy(outputs.detach().cpu().numpy(),labels.to('cpu').numpy())\n           \n            y_true.append(labels.to('cpu').numpy())\n            y_pred.append(outputs.detach().cpu().numpy())\n            \n        \n    loss =  epoch_loss / len(loader)\n    acc = epoch_acc / len(loader)  \n    return loss, acc, y_pred, y_true","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:02:50.183553Z","iopub.execute_input":"2022-01-27T11:02:50.183808Z","iopub.status.idle":"2022-01-27T11:02:50.192833Z","shell.execute_reply.started":"2022-01-27T11:02:50.183778Z","shell.execute_reply":"2022-01-27T11:02:50.192125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run Model","metadata":{}},{"cell_type":"code","source":"def run_model(model, train_loader, validate_loader, test_loader, epochs, batch_size, optimizer):\n   \n    for epoch in range(epochs):\n        train_loss, train_acc = train_model(model, optimizer, train_loader)\n        valid_loss, valid_acc, _, _ = evaluate_model(model, validate_loader)\n\n        print(f'Epoch: {epoch+1:02}')\n        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n\n    ##Evaluate the test accuracy\n\n    test_loss, test_acc, y_pred, y_true = evaluate_model(model, test_loader)\n    print(f'\\nTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n    \n    flat_pred = np.concatenate(y_pred,axis=0)\n    flat_pred = np.argmax(flat_pred,axis=1).flatten()\n    flat_true = np.concatenate(y_true, axis=0)\n    print(\"\\nConfusion Matrix : \\n\",confusion_matrix(flat_true,flat_pred))\n    print(\"\\nClassification Report : \\n\", classification_report(flat_true,flat_pred))\n    \nNUM_EPOCHS = 1\nbatch_size = 20","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:02:53.056098Z","iopub.execute_input":"2022-01-27T11:02:53.057001Z","iopub.status.idle":"2022-01-27T11:02:53.064661Z","shell.execute_reply.started":"2022-01-27T11:02:53.056963Z","shell.execute_reply":"2022-01-27T11:02:53.063995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# RUN BERT Model\nprint(\"\\n\\nBERT Model :\\n\")\ndataset_bert = TensorDataset(sentenceID, inputID_bert, attentionMask_bert, labels)\ntrain_loader_bert, validation_loader_bert, test_loader_bert = get_loaders(dataset_bert, batch_size, 1)\noptimizer_bert = AdamW(model_bert.parameters(),lr=5e-5,eps=1e-8)\n\nrun_model(model_bert, train_loader_bert, validation_loader_bert, test_loader_bert, NUM_EPOCHS, batch_size, optimizer_bert)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:03:01.592636Z","iopub.execute_input":"2022-01-27T11:03:01.593356Z","iopub.status.idle":"2022-01-27T11:11:40.903557Z","shell.execute_reply.started":"2022-01-27T11:03:01.593314Z","shell.execute_reply":"2022-01-27T11:11:40.90284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# RUN RoBERTa Model\nprint(\"\\n\\nRoBERTa Model :\\n\")\ndataset_roberta = TensorDataset(inputID_roberta, attentionMask_roberta, labels)\ntrain_loader_roberta, validation_loader_roberta, test_loader_roberta = get_loaders(dataset_roberta, batch_size, 0)\noptimizer_roberta = AdamW(model_roberta.parameters(),lr=5e-5,eps=1e-8)\n\nrun_model(model_roberta, train_loader_roberta, validation_loader_roberta, test_loader_roberta, NUM_EPOCHS, batch_size, optimizer_roberta)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:19:23.297075Z","iopub.execute_input":"2022-01-27T11:19:23.297828Z","iopub.status.idle":"2022-01-27T11:28:05.655931Z","shell.execute_reply.started":"2022-01-27T11:19:23.297787Z","shell.execute_reply":"2022-01-27T11:28:05.655207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summary","metadata":{}},{"cell_type":"markdown","source":"Case 1 : \nBag of Words : Set of vectors containing count of word occurences, a simple and flexible approach in Text Classification.\nSklearn Library : SVM and RandomForest : Optimal Classifiers for Binary Classification.\n\nCase 2 :\nTF-IDF : It assigns a value to a term according to its importance in the text scaled by its importance across all the texts in the data. A popular approach in NLP.\nXGBoost and LightGBM : Both are based on Gradient Boosted Decision Trees. In XGBoost, trees grow depth-wise and in LightGBM, trees grow leaf-wise. Both models had great success in enterprise applications and data science competitions. XGBoost is extremely powerful, though model training is faster in LightGBM.\n\nCase 3 :\nPre-trained GloVe Embedding : GloVe = Global vectors for word representation. It is an unsupervised algorithm developed by Standford for generating word embeddings by aggregating global word-word co-occurence matrix from a corpus, which gives semantic relationships between words. Here, I have user Pretrained Word Vector of Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB) from https://nlp.stanford.edu/projects/glove/\nTensorflow Framework : Bi-Directional LSTM : LSTM is classic model used for NLP tasks\n\nCase 4 :\nPyTorch Framework : HuggingFace transformers Library\nBERT : Google's BERT (October-2018) is the transformer based method for NLP, outperforming state-of-the-art on several tasks such as QnA, language inference. It is a pre-trained deep Bi-directional Encoder Representation from transformer with Masked Language Modelling (MLM) and Next Sentence Prediction (NSP).\nRoBERTa : Facebook's RoBERTa (July-2019), robustly optimized BERT approach, advancing the state-of-the-art in self-supervised systems. It is a BERT without Next Sentence Prediction (NSP). To improve training procedure, RoBERTa removes the Next Sentence Prediction (NSP) task from BERT's pre-training and dynamic masking so that the masked token changes during training epochs.\n\nThe Most Preferred Model : From these 4 cases, currently the RoBERTa model is the most preferred one, as it is the optimized BERT approach.","metadata":{}}]}